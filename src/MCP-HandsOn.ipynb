{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3616282",
   "metadata": {},
   "source": [
    "## í™˜ê²½ì„¤ì •\n",
    "\n",
    "ì•„ë˜ ì„¤ì¹˜ ë°©ë²•ì„ ì°¸ê³ í•˜ì—¬ `uv` ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**uv ì„¤ì¹˜ ë°©ë²•**\n",
    "\n",
    "```bash\n",
    "# macOS/Linux\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Windows (PowerShell)\n",
    "irm https://astral.sh/uv/install.ps1 | iex\n",
    "```\n",
    "\n",
    "**ì˜ì¡´ì„± ì„¤ì¹˜**\n",
    "\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f439824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"/mnt/hdd/hyeontae/langgraph-mcp-agents/.env\",\n",
    "            override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90853355",
   "metadata": {},
   "source": [
    "## MultiServerMCPClient\n",
    "\n",
    "`async with`ë¡œ ì¼ì‹œì ì¸ Session ì—°ê²°ì„ ìƒì„± í›„ í•´ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf7fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x7f455f63a8e0>)]\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in ì„œìš¸\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì„œìš¸ì€ í•­ìƒ í™”ì°½í•©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "client =  MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # ì„œë²„ì˜ í¬íŠ¸ì™€ ì¼ì¹˜í•´ì•¼ í•œë‹¤.\n",
    "            \"url\": \"http://localhost:8005/sse\", # localì—ì„œ ì„œë²„ ì˜¬ë ¤ì„œ 8005ë²ˆê³¼ í†µì‹ í•˜ê² ë‹¤.\n",
    "            \"transport\": \"sse\", # sse ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(await client.get_tools())\n",
    "agent = create_react_agent(model, tools=await client.get_tools())\n",
    "answer = await astream_graph(agent, {\"messages\" : \"ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930407c3",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” sessionì´ ë‹«í˜”ê¸° ë•Œë¬¸ì— ë„êµ¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb63613",
   "metadata": {},
   "source": [
    "## Stdio í†µì‹  ë°©ì‹\n",
    "\n",
    "Stdio í†µì‹  ë°©ì‹ì€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- í†µì‹ ì„ ìœ„í•´ í‘œì¤€ ì…ë ¥/ì¶œë ¥ ì‚¬ìš©\n",
    "\n",
    "ì°¸ê³ : ì•„ë˜ì˜ python ê²½ë¡œëŠ” ìˆ˜ì •í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# - command : Python ì¸í„°í”„ë¦¬í„° ê²½ë¡œ\n",
    "# - args : ì‹¤í–‰í•  MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸\n",
    "server_params = StdioServerParameters(\n",
    "    command = \"/mnt/hdd/Anaconda3/mcp/bin/python\",\n",
    "    args = [\"mcp_server_local.py\"]\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # ì—°ê²° ì´ˆê¸°í™”\n",
    "        await session.initialize()\n",
    "        \n",
    "        # MCP ë„êµ¬ ë¡œë“œ\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "        \n",
    "        # ì—ì´ì „íŠ¸ ìƒì„±\n",
    "        agent = create_react_agent(model, tools=tools)\n",
    "        \n",
    "        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        await astream_graph(agent,\n",
    "                           {\"messages\": \"ì„œì„ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27867a3d",
   "metadata": {},
   "source": [
    "## RAG ë¥¼ êµ¬ì¶•í•œ MCP ì„œë²„ ì‚¬ìš©\n",
    "\n",
    "- íŒŒì¼: `mcp_server_rag.py`\n",
    "\n",
    "ì‚¬ì „ì— langchain ìœ¼ë¡œ êµ¬ì¶•í•œ `mcp_server_rag.py` íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "stdio í†µì‹  ë°©ì‹ìœ¼ë¡œ ë„êµ¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œ ë„êµ¬ëŠ” `retriever` ë„êµ¬ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë˜ë©°, ì´ ë„êµ¬ëŠ” `mcp_server_rag.py` ì—ì„œ ì •ì˜ëœ ë„êµ¬ì…ë‹ˆë‹¤. ì´ íŒŒì¼ì€ ì‚¬ì „ì— ì„œë²„ì—ì„œ ì‹¤í–‰ë˜ì§€ **ì•Šì•„ë„** ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import astream_graph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# RAG ì„œë²„ë¥¼ ìœ„í•œ StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"/mnt/hdd/Anaconda3/mcp/bin/python\",\n",
    "    args=[\"mcp_server_rag.py\"]\n",
    ")\n",
    "\n",
    "# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì„œë²„ì™€ í†µì‹ \n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # ì—°ê²° ì´ˆê¸°í™”\n",
    "        await session.initialize()\n",
    "        \n",
    "        # MCP ë„êµ¬ ë¡œë“œ ( ì—¬ê¸°ì„œëŠ” Retriever ë„êµ¬ )\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰\n",
    "        agent = create_react_agent(model, tools=tools)\n",
    "        \n",
    "        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°\n",
    "        await astream_graph(\n",
    "            agent,\n",
    "            {\"messages\": \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af805d",
   "metadata": {},
   "source": [
    "## SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ í˜¼í•© ì‚¬ìš©\n",
    "\n",
    "- íŒŒì¼: `mcp_server_rag.py` ëŠ” StdIO ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "- `langchain-dev-docs` ëŠ” SSE ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "\n",
    "SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ac52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 1. ë‹¤ì¤‘ ì„œë²„ MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"/mnt/hdd/Anaconda3/mcp/bin/python\",\n",
    "            \"args\": [\"mcp_server_rag.py\"],\n",
    "            # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \n",
    "        \"langchain-dev-docs\": {\n",
    "            \"url\": \"https://teddynote.io/mcp/langchain/sse\",\n",
    "            # SSE(Server-Sent Events) ë°©ì‹ìœ¼ë¡œ í†µì‹ \n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ëª…ì‹œì  ì—°ê²° ì´ˆê¸°í™”\n",
    "await client.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8010073",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eaa7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt = \"\"\"You are a smart agent.\n",
    "Use 'retrieve' tool to search AI related documents and answer questions.\n",
    "Use 'langchain-dev-docs' tools to search on langchain / langgraph related documents and answer questino.\n",
    "Answer in Korean.\"\"\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model, \n",
    "    await client.get_tools(), \n",
    "    prompt=prompt, \n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aafe27",
   "metadata": {},
   "source": [
    "êµ¬ì¶•í•´ ë†“ì€ `mcp_server_rag.py` ì—ì„œ ì •ì˜í•œ `retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"retrieve ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜.\"\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a59c65",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” `langchain-dev-docs` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c904ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\"messages\": \"langgraph-dev-docs ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ self-rag ì½”ë“œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”(langgraphë¡œ).\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ea107",
   "metadata": {},
   "source": [
    "`MemorySaver` ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê¸° ê¸°ì–µì„ ìœ ì§€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, multi-turn ëŒ€í™”ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbafcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent, {\"messages\": \"ì´ì „ì˜ ë‚´ìš©ì„ bullet point ë¡œ ìš”ì•½í•´ì¤˜\"}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c2531",
   "metadata": {},
   "source": [
    "## LangChain ì— í†µí•©ëœ ë„êµ¬ + MCP ë„êµ¬\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” LangChain ì— í†µí•©ëœ ë„êµ¬ë¥¼ ê¸°ì¡´ì˜ MCP ë¡œë§Œ ì´ë£¨ì–´ì§„ ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©ì´ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec97e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (news íƒ€ì…, ìµœê·¼ 3ì¼ ë‚´ ë‰´ìŠ¤)\n",
    "tavily = TavilySearchResults(max_results=3, topic=\"news\", days=3)\n",
    "\n",
    "# ê¸°ì¡´ì˜ MCP ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "tools = await client.get_tools() + [tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff4617",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ccc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=2)\n",
    "\n",
    "prompt = \"You are a smart agent with various tools. Answer questions in Korean.\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    prompt=prompt,\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e2e50",
   "metadata": {},
   "source": [
    "ìƒˆë¡­ê²Œ ì¶”ê°€í•œ `tavily` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2eb907",
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent, \n",
    "    {\"messages\": \"ì•„ì´í° 17 ê´€ë ¨í•œ ì˜¤ëŠ˜ ë‰´ìŠ¤ ì°¾ì•„ì¤˜\"},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86700ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retrieve` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71b512",
   "metadata": {},
   "source": [
    "## Smithery ì—ì„œ ì œê³µí•˜ëŠ” MCP ì„œë²„\n",
    "\n",
    "- ë§í¬: https://smithery.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741505d",
   "metadata": {},
   "source": [
    "ì‚¬ìš©í•œ ë„êµ¬ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking\n",
    "  - êµ¬ì¡°í™”ëœ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì—­ë™ì ì´ê³  ì„±ì°°ì ì¸ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” MCP ì„œë²„\n",
    "- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander\n",
    "  - ë‹¤ì–‘í•œ í¸ì§‘ ê¸°ëŠ¥ìœ¼ë¡œ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  íŒŒì¼ì„ ê´€ë¦¬í•˜ì„¸ìš”. ì½”ë”©, ì…¸ ë° í„°ë¯¸ë„, ì‘ì—… ìë™í™”\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- smithery ì—ì„œ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¬ë•Œ, ì•„ë˜ì˜ ì˜ˆì‹œì²˜ëŸ¼ `\"transport\": \"stdio\"` ë¡œ ê¼­ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9907506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mcp_config():\n",
    "    \"\"\"í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ MCP ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with open(\"./config.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"ì„¤ì • íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_server_config():\n",
    "    \"\"\"MCP ì„œë²„ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    config = load_mcp_config()\n",
    "    server_config = {}\n",
    "\n",
    "    if config and \"mcpServers\" in config:\n",
    "        for server_name, server_config_data in config[\"mcpServers\"].items():\n",
    "            # commandê°€ ìˆìœ¼ë©´ stdio ë°©ì‹\n",
    "            if \"command\" in server_config_data:\n",
    "                server_config[server_name] = {\n",
    "                    \"command\": server_config_data.get(\"command\"),\n",
    "                    \"args\": server_config_data.get(\"args\", []),\n",
    "                    \"transport\": \"stdio\",\n",
    "                }\n",
    "            # urlì´ ìˆìœ¼ë©´ sse ë°©ì‹\n",
    "            elif \"url\" in server_config_data:\n",
    "                server_config[server_name] = {\n",
    "                    \"url\": server_config_data.get(\"url\"),\n",
    "                    \"transport\": \"sse\",\n",
    "                }\n",
    "\n",
    "    return server_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0b9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"/mnt/hdd/hyeontae/langgraph-mcp-agents/.env\",\n",
    "            override=True)\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "server_config = create_server_config()\n",
    "client = MultiServerMCPClient(server_config)\n",
    "\n",
    "# agent = create_react_agent(model, await client.get_tools())\n",
    "\n",
    "# query =\"í˜„ì¬ ê²½ë¡œë¥¼ í¬í•¨í•œ í•˜ìœ„ í´ë” êµ¬ì¡°ë¥¼ tree ë¡œ ê·¸ë ¤ì¤˜. ë‹¨, .venv í´ë”ëŠ” ì œì™¸í•˜ê³  ì¶œë ¥í•´ì¤˜.\"\n",
    "\n",
    "# response = await agent.invoke({\"messages\":query})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1085a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"/mnt/hdd/hyeontae/langgraph-mcp-agents/.env\",\n",
    "            override=True)\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1024,\n",
    "    google_api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "# 1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client_x = MultiServerMCPClient(\n",
    "    {\n",
    "        \"desktop-commander\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@wonderwhy-er/desktop-commander\",\n",
    "                \"--key\",\n",
    "                \"c8974619-e333-499e-ae13-3c6e7e5c0c31\"\n",
    "            ],\n",
    "            \"transport\" : \"stdio\"\n",
    "        },\n",
    "    \n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"/mnt/hdd/Anaconda3/mcp/bin/python\",\n",
    "            # mcp_server_rag.py íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# await client.__aenter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9358a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client_x.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b72dcc",
   "metadata": {},
   "source": [
    "langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d942b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=3)\n",
    "agent = create_react_agent(model, await client.get_tools(), checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d132a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
